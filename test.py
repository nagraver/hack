from pathlib import Path  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
import os
import requests
import gradio as gr
import logging
from typing import Optional, List
from uuid import uuid4
import subprocess
import tempfile

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import SentenceTransformersTokenTextSplitter
from langchain_core.language_models.llms import LLM
from langchain_core.documents import Document
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "code-assistant-store")


class LMStudioLLM(LLM):
    model: str = "local-model"
    base_url: str = "http://127.0.0.1:1234/v1"
    temperature: float = 0.7
    max_tokens: int = 1024
    top_p: float = 0.9
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
        }
        response = requests.post(f"{self.base_url}/chat/completions", json=payload)
        return response.json()["choices"][0]["message"]["content"]

    @property
    def _llm_type(self) -> str:
        return "lmstudio-llm"


def configure_qdrant() -> QdrantVectorStore:
    """Configure and return a Qdrant vector store."""
    logger.info("Configuring Qdrant client and vector store...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3", encode_kwargs={"truncation": True, "max_length": 512})
    embeddings.embed_query("test")  # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant
    client = QdrantClient(path="/tmp/langchain_qdrant")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    try:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
        )
    except ValueError:
        logger.info("Collection already exists. Skipping creation.")

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
    )

    logger.info("Qdrant configuration completed.")
    return vector_store


def clone_github_repo(repo_url: str, temp_dir: str) -> str:
    """Clone GitHub repository to temporary directory."""
    try:
        repo_name = repo_url.split("/")[-1]
        if repo_name.endswith(".git"):
            repo_name = repo_name[:-4]
        repo_path = os.path.join(temp_dir, repo_name)

        subprocess.run(["git", "clone", repo_url, repo_path], check=True)
        logger.info(f"Repository cloned successfully to {repo_path}")
        return repo_path
    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to clone repository: {e}")
        raise


def load_documents_from_repo(repo_path: str) -> List[Document]:
    """Load documents from repository."""
    documents = []

    for root, _, files in os.walk(repo_path):
        for file in files:
            if file.endswith((".txt", ".md", ".py", ".js", ".java", ".cpp", ".h", ".html", ".css")):
                file_path = os.path.join(root, file)
                try:
                    loader = TextLoader(file_path, encoding="utf-8")
                    loaded_docs = loader.load()
                    for doc in loaded_docs:
                        doc.metadata["source_file"] = file_path  # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    documents.extend(loaded_docs)
                    logger.info(f"Loaded {file_path}")
                except Exception as e:
                    logger.warning(f"Could not load {file_path}: {e}")

    return documents


def add_repository_to_store(
    vector_store: QdrantVectorStore, repo_url: str, chunk_size: int = 512, chunk_overlap: int = 50
) -> None:
    """Add repository documents to vector store."""
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
            repo_path = clone_github_repo(repo_url, temp_dir)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            documents = load_documents_from_repo(repo_path)

            if not documents:
                raise ValueError("No documents found in repository")

            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
            splitter = SentenceTransformersTokenTextSplitter(
                tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name="BAAI/bge-m3"
            )
            texts = splitter.split_documents(documents)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            ids = [str(uuid4()) for _ in texts]
            vector_store.add_documents(documents=texts, ids=ids)
            logger.info(f"Added {len(texts)} documents from repository to vector store")

        except Exception as e:
            logger.error(f"Error processing repository: {e}")
            raise


def load_or_create_vector_store(
    vector_store: QdrantVectorStore, file_path: str = None, chunk_size: int = 512, chunk_overlap: int = 50
) -> QdrantVectorStore:
    """Load or create vector store with documents."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    client = vector_store.client
    collection_info = client.get_collection(COLLECTION_NAME)

    if collection_info.vectors_count == 0 and file_path:
        logger.info("No documents in vector store. Creating new index...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        loader = TextLoader(file_path)
        documents = loader.load()
        for doc in documents:
            doc.metadata["source_file"] = file_path  # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏
        splitter = SentenceTransformersTokenTextSplitter(
            tokens_per_chunk=chunk_size, chunk_overlap=chunk_overlap, model_name="BAAI/bge-m3"
        )
        texts = splitter.split_documents(documents)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        ids = [str(uuid4()) for _ in texts]
        vector_store.add_documents(documents=texts, ids=ids)
        logger.info(f"Added {len(texts)} documents to vector store.")
    else:
        logger.info(f"Vector store already contains {collection_info.vectors_count} vectors. Using existing index.")

    return vector_store


# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ===
llm = LMStudioLLM()
vector_store = configure_qdrant()

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
FILE_PATH = r"./my_notes.txt"
if os.path.exists(FILE_PATH):
    vector_store = load_or_create_vector_store(vector_store, FILE_PATH)
else:
    vector_store = load_or_create_vector_store(vector_store)


# === –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Gradio ===
def answer_question(question, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda):
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LLM
    llm.temperature = temperature
    llm.max_tokens = max_tokens
    llm.top_p = top_p

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    retriever = vector_store.as_retriever(
        search_type="mmr", search_kwargs={"k": k_results, "fetch_k": fetch_k, "lambda_mult": mmr_lambda}
    )

    # –°–±–æ—Ä–∫–∞ —Ü–µ–ø–æ—á–∫–∏ —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt_template},
        return_source_documents=True,
    )

    response = qa_chain({"query": question})

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    answer = response["result"]
    source_docs = response["source_documents"]

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    source_files = set()
    for doc in source_docs:
        if "source_file" in doc.metadata:
            source_files.add(doc.metadata["source_file"])

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    if source_files:
        answer += "\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n- " + "\n- ".join(source_files)

    return answer


def add_repository(repo_url, chunk_size, chunk_overlap):
    try:
        add_repository_to_store(vector_store, repo_url, chunk_size, chunk_overlap)
        return "–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω!"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è: {str(e)}"


# === –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç ===
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫–∞—Å–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–µ–∫—Ç–∞\n\n"
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
    ),
)

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio ===
# ... (–≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∏–º–ø–æ—Ä—Ç—ã –æ—Å—Ç–∞—é—Ç—Å—è —Ç–∞–∫–∏–º–∏ –∂–µ)

# ... (–≤–µ—Å—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–¥ –¥–æ –±–ª–æ–∫–∞ —Å Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio ===
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å–æ —Å—Ç–∏–ª—è–º–∏
css_file = Path("./gradio_style.css")
custom_css = css_file.read_text() if css_file.exists() else ""

with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("## üß† –õ–æ–∫–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∫–æ–¥—É")
    gr.Markdown("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLaMA 3.1 —á–µ—Ä–µ–∑ LM Studio + LangChain + Qdrant + bge-m3")

    with gr.Tabs():
        with gr.Tab("–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å", id="question_tab"):
            with gr.Row():
                with gr.Column():
                    question_input = gr.Textbox(lines=2, placeholder="–ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –ø–æ –ø—Ä–æ–µ–∫—Ç—É...", label="–í–æ–ø—Ä–æ—Å")
                    with gr.Accordion("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏", open=False):
                        temperature = gr.Slider(0.1, 2.0, value=0.7, step=0.1, label="Temperature")
                        max_tokens = gr.Slider(128, 4096, value=1024, step=128, label="Max Tokens")
                        top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.1, label="Top-p")
                    with gr.Accordion("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞", open=False):
                        k_results = gr.Slider(1, 10, value=5, step=1, label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                        fetch_k = gr.Slider(5, 50, value=20, step=5, label="Fetch K (–∫–æ–ª-–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)")
                        mmr_lambda = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label="MMR Lambda (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)")
                    with gr.Row():
                        ask_button = gr.Button("–°–ø—Ä–æ—Å–∏—Ç—å", variant="primary")
                        clear_btn = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å", variant="secondary")
                with gr.Column():
                    answer_output = gr.Textbox(label="–û—Ç–≤–µ—Ç", interactive=False)

        with gr.Tab("–î–æ–±–∞–≤–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π", id="repo_tab"):
            with gr.Row():
                with gr.Column():
                    repo_url_input = gr.Textbox(
                        placeholder="https://github.com/username/repository.git", label="URL GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"
                    )
                    with gr.Accordion("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞–Ω–∫–∏–Ω–≥–∞", open=False):
                        chunk_size = gr.Slider(128, 1024, value=512, step=64, label="–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (—Ç–æ–∫–µ–Ω—ã)")
                        chunk_overlap = gr.Slider(0, 256, value=50, step=16, label="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ (—Ç–æ–∫–µ–Ω—ã)")
                    with gr.Row():
                        add_repo_button = gr.Button("–î–æ–±–∞–≤–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π", variant="primary")
                        clear_repo_btn = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å", variant="secondary")
                with gr.Column():
                    repo_status_output = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", interactive=False)

        with gr.Tab("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", id="info_tab"):
            gr.Markdown("### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            gr.Markdown(
                """
            1. **–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å** - –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ –≤–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ
            2. **–î–æ–±–∞–≤–∏—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π** - –∑–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–æ–≤—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∏—Å–∫–∞ –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            """
            )
            gr.Markdown("### –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏")
            gr.Markdown(
                """
            - **LM Studio** - –ª–æ–∫–∞–ª—å–Ω—ã–π LLM
            - **LangChain** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
            - **Qdrant** - –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            - **bge-m3** - –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
            """
            )

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–Ω–æ–ø–æ–∫ –æ—á–∏—Å—Ç–∫–∏
    clear_btn.click(
        lambda: [None, 0.7, 1024, 0.9, 5, 20, 0.5],
        outputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],
    )
    clear_repo_btn.click(lambda: [None, 512, 50], outputs=[repo_url_input, chunk_size, chunk_overlap])

    ask_button.click(
        answer_question,
        inputs=[question_input, temperature, max_tokens, top_p, k_results, fetch_k, mmr_lambda],
        outputs=answer_output,
    )
    add_repo_button.click(
        add_repository, inputs=[repo_url_input, chunk_size, chunk_overlap], outputs=repo_status_output
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
